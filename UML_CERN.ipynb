{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc81468",
   "metadata": {},
   "source": [
    "<h1>Welcome to the accompanying Jupyter Notebook to explore how pre-processing and algorithm choices in UML affect the results and their interpretability and representativeness<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7610de",
   "metadata": {},
   "source": [
    "This Notebook allows you to change and tweak pre-processing and parameters to your liking to see what changes. As a result two Excel-files are created, one for assessing interpretability and one for representativeness. There is a dataset provided here, but you can upload your own if you so wish. \n",
    "\n",
    "\n",
    "Let us begin by importing general Python data analysis packages and Spacy - the natural language processing library we will be using. To install spacy, visit https://spacy.io/usage. We recommend using Anaconda to manage Python environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84eef7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a83ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 330.3 kB/s eta 0:00:39\n",
      "     --------------------------------------- 0.1/12.8 MB 469.7 kB/s eta 0:00:28\n",
      "     - -------------------------------------- 0.3/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 1.0/12.8 MB 4.8 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 2.0/12.8 MB 7.9 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.1/12.8 MB 10.2 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 4.3/12.8 MB 12.4 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 14.1 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.3/12.8 MB 14.9 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.4/12.8 MB 15.2 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.6/12.8 MB 16.1 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.7/12.8 MB 16.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.8/12.8 MB 23.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 11.7/12.8 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 22.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f55269",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model = \"en_core_web_sm\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cecdd8e",
   "metadata": {},
   "source": [
    "Uploading the provided data sample to a pandas dataframe and implementing some basic data cleansing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b68297",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('cern_news_data.xlsx')\n",
    "\n",
    "#Choosing the columns we want for our analysis\n",
    "df = df[['Document', 'Label']]\n",
    "\n",
    "#Dropping rows with empty data\n",
    "df = df.dropna(how = 'any',axis = 0).reset_index(drop = True)\n",
    "#Dropping rows of duplicate text documents\n",
    "df = df.drop_duplicates(subset=\"Document\")\n",
    "\n",
    "#If you want to take a smaller sample of the data for faster exploration, uncomment the following line:\n",
    "#df = df.head(550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b29c26",
   "metadata": {},
   "source": [
    "More basic cleansing steps are applied to the text documents in the 'Action'-column of the dataframe. \n",
    "Extra whitespace and asterisks marks are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "210d5e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a new list of text documents with extra whitespace and asterisks and quotation marks \n",
    "#that would complicate further cleansing removed\n",
    "cleaner_documents = [text.replace(\"*\", \" \").replace('\"','') for text in list(df['Document'])]\n",
    "clean_documents = [re.sub('[\\s+]', ' ',text) for text in cleaner_documents]\n",
    "\n",
    "#Adding the cleansed documents to the dataframe\n",
    "df['Action Clean'] = clean_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ac0d8",
   "metadata": {},
   "source": [
    "Let us take a look at a sample of the clean documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad223afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Founded in 2004, Zecotek operates three divisions: Imaging Systems, Optronics Systems and 3D Display Systems with labs located in Canada, Korea, Russia, Singapore and U.S.A. The management team is focused on building shareholder value by commercializing over 50 patented and patent pending novel photonic technologies directly and through strategic alliances with Hamamatsu Photonics (Japan), the European Organization for Nuclear Research (Switzerland), Shanghai EBO Optoelectronics Technology Co. (China), NuCare Medical Systems (South Korea), the University of Washington (United States), and National NanoFab Center (South Korea). For more information visit www.zecotek.com and follow @zecotek on Twitter.\n",
      "\n",
      "Pakistan has a long tradition of international scientific collaborations. In addition to being actively involved in IAEA's activities, for decades Pakistan has been contributing and regularly participating in European Organization for Nuclear Research's projects, theoretical and nuclear experiments,' she said. Pakistan, she said, became the first country in the region to gain Associate Membership of CERN in 2014.\n",
      "\n",
      " Berners-Lee worked at the European Organization for Nuclear Research (CERN - it's a French thing). By this time, he had created the first web server, the first web browser, and the first webpage, along with the HyperText Markup Language (HTML) and the HyperText Transfer Protocol (HTTP).\n",
      "\n",
      "In others, it is impossible to remember life before immediate access, buying living room furniture on your phone and bellyaching about WiFi access. On that April Friday in 1993, CERN, the short name for the European Organization for Nuclear Research, released the codes to the public. It made it available to everyone and offered endless possibilities of technology to us.\n",
      "\n",
      "Doha   TWO Qatari electrical engineering students, who are graduating from Texas A&M University at Qatar (TAMUQ) this year, have spoken of their life-changing internships with CERN, the largest research center in the world for high energy physics. Qatari brother and sister Abdulaziz al Qahtani and Shaikha al Qahtani spent eight weeks in CERN, the European Organization for Nuclear Research. Joining the day-to-day work of research teams participating in cutting-edge experiments in Geneva, Switzerland, the siblings say they are determined to further engage in research-driven programs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clean_document in clean_documents[:5]:\n",
    "    print(clean_document+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fb379b",
   "metadata": {},
   "source": [
    "<h2>Make pre-processing choices.<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae257ada",
   "metadata": {},
   "source": [
    "Set the pre-processing step you wish to employ as 'True' and others as 'False'.\n",
    "A warning is raised if you choose conflicting pre-processing.\n",
    "However, if you choose 'True' for both chunks and n-grams, the program will make n-grams of chunks.\n",
    "\n",
    "Consult the table on pre-processing choices in the article for details on these choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "674ac274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "\n",
    "#Tokenization:\n",
    "basic_tokenization = False\n",
    "chunk_tokenization = False\n",
    "ngram_tokenization = True\n",
    "#When using n-grams, how many adjacent tokens to gram together:\n",
    "n = 2\n",
    "\n",
    "#If conflicting choices, raise warning.\n",
    "if sum(map(bool, [basic_tokenization,chunk_tokenization,ngram_tokenization])) != 1:\n",
    "    warnings.warn(\"Please specify exactly one tokenization.\")\n",
    "\n",
    "#Vectorization:\n",
    "bow_vectorization = False\n",
    "tf_idf_vectorization = True\n",
    "\n",
    "#If conflicting choices, raise warning.\n",
    "if sum(map(bool, [bow_vectorization, tf_idf_vectorization])) != 1:\n",
    "    warnings.warn(\"Please specify exactly one vectorization.\")\n",
    "\n",
    "#Lemmatization:\n",
    "lemmatized = True\n",
    "\n",
    "#If chunk tokenization chosen, Spacy will use chunks as tokens\n",
    "if chunk_tokenization:\n",
    "    nlp = spacy.load(spacy_model)\n",
    "    nlp.add_pipe(\"merge_noun_chunks\")\n",
    "#If chunk tokenization not chosen, chunk retrieval will be disabled from Spacy\n",
    "else:\n",
    "    nlp = spacy.load(spacy_model, disable=['merge_noun_chunks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c04aded",
   "metadata": {},
   "source": [
    "The following function is for naming output-files according to the pre-processing regime used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6d3feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "#Get the date of today to add to file names for file version control\n",
    "today = str(datetime.now().date())\n",
    "\n",
    "def pre_processing_name():\n",
    "    if basic_tokenization:\n",
    "        tokenization_name = \"Basic\"\n",
    "    if chunk_tokenization:\n",
    "        tokenization_name = \"Chunk\"\n",
    "    if ngram_tokenization:\n",
    "        tokenization_name = \"Ngram\"\n",
    "    if bow_vectorization:\n",
    "        vectorizer_name = \"BOW\"\n",
    "    if tf_idf_vectorization:\n",
    "        vectorizer_name = \"TFIDF\"\n",
    "    \n",
    "    #Not a string\n",
    "    return (tokenization_name, vectorizer_name, today)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170efd95",
   "metadata": {},
   "source": [
    "Using Spacy to tokenize the text documents. Here every token is lowercased, and if lemmatization is chosen, also Spacy's lemmas will be used. Known entities, stopwords, punctuation, spaces, numerals, urls and emails will be removed in cleansing. If any of these want to be retained, simply remove the \"token.(what_you_want_to_keep)\" from the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0b4830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_tokenizer(document, lemmatized=lemmatized):\n",
    "    #Converting the text document into a Spacy document\n",
    "    document = nlp(document)\n",
    "    if not lemmatized:\n",
    "        tokenized = [token.text.lower() for token in document if token.ent_iob == 2 #<- This removes known entities\n",
    "                     and not (token.is_stop or token.is_punct or token.is_space or token.like_num \n",
    "                              or token.like_url or token.like_email)]\n",
    "    if lemmatized:\n",
    "        tokenized = [token.lemma_.lower() for token in document if token.ent_iob == 2 #<- This removes known entities\n",
    "                     and not (token.is_stop or token.is_punct or token.is_space or token.like_num \n",
    "                              or token.like_url or token.like_email)]       \n",
    "    #Returns a list of tokens\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6095803",
   "metadata": {},
   "source": [
    "The following function does exactly what basic_tokenizer does, but combines the tokens into n-grams according to the set n-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70018985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_tokenizer(document, n=n, lemmatized=lemmatized):\n",
    "    #Converting the text document into a Spacy document\n",
    "    document = nlp(document)\n",
    "    if not lemmatized:\n",
    "        tokenized = [token.text.lower() for token in document if token.ent_iob == 2 \n",
    "                     and not (token.is_stop or token.is_punct or token.is_space or token.like_num \n",
    "                              or token.like_url or token.like_email)]\n",
    "        #Joins found tokens with \"_\" into n-grams\n",
    "        ngrams = [\"_\".join(ngram) for ngram in zip(*[tokenized[i:] for i in range(n)])]\n",
    "    if lemmatized:\n",
    "        tokenized = [token.lemma_.lower() for token in document if token.ent_iob == 2 \n",
    "                     and not (token.is_stop or token.is_punct or token.is_space or token.like_num \n",
    "                              or token.like_url or token.like_email)]       \n",
    "        #Joins found tokens with \"_\" into n-grams\n",
    "        ngrams = [\"_\".join(ngram) for ngram in zip(*[tokenized[i:] for i in range(n)])]\n",
    "    #Returns a list of tokens\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5b1e4f",
   "metadata": {},
   "source": [
    "Initializing an empty list for tokenized documents to be added to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6066727",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_documents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa2fa2",
   "metadata": {},
   "source": [
    "Tokenizing the cleansed text documents according to the pre-processing choices and adding the tokenized document to the initalized list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74f790b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in clean_documents:\n",
    "    if chunk_tokenization:\n",
    "        #using basic tokenizer on the document with Spacy's chunks enabled\n",
    "        tokenized = basic_tokenizer(document)\n",
    "    if basic_tokenization:\n",
    "        #using basic tokenizer on the document with Spacy's chunks disabled\n",
    "        tokenized = basic_tokenizer(document)\n",
    "    if ngram_tokenization:\n",
    "        tokenized = ngram_tokenizer(document)\n",
    "    #adding the tokenized document to tokenized_documents\n",
    "    tokenized_documents.append(tokenized)\n",
    "\n",
    "#Adding the tokenized documents to the dataframe\n",
    "df['Tokenized'] = tokenized_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f438897",
   "metadata": {},
   "source": [
    "Let us take a look at the tokenized documents compared to the cleansed ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b59821e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleansed: Founded in 2004, Zecotek operates three divisions: Imaging Systems, Optronics Systems and 3D Display Systems with labs located in Canada, Korea, Russia, Singapore and U.S.A. The management team is focused on building shareholder value by commercializing over 50 patented and patent pending novel photonic technologies directly and through strategic alliances with Hamamatsu Photonics (Japan), the European Organization for Nuclear Research (Switzerland), Shanghai EBO Optoelectronics Technology Co. (China), NuCare Medical Systems (South Korea), the University of Washington (United States), and National NanoFab Center (South Korea). For more information visit www.zecotek.com and follow @zecotek on Twitter.\n",
      "\n",
      "Tokenized: ['found_operate', 'operate_division', 'division_lab', 'lab_locate', 'locate_management', 'management_team', 'team_focus', 'focus_build', 'build_shareholder', 'shareholder_value', 'value_commercialize', 'commercialize_patented', 'patented_patent', 'patent_pende', 'pende_novel', 'novel_photonic', 'photonic_technology', 'technology_directly', 'directly_strategic', 'strategic_alliance', 'alliance_information', 'information_visit', 'visit_follow', 'follow_@zecotek']\n",
      "\n",
      "Cleansed: Pakistan has a long tradition of international scientific collaborations. In addition to being actively involved in IAEA's activities, for decades Pakistan has been contributing and regularly participating in European Organization for Nuclear Research's projects, theoretical and nuclear experiments,' she said. Pakistan, she said, became the first country in the region to gain Associate Membership of CERN in 2014.\n",
      "\n",
      "Tokenized: ['long_tradition', 'tradition_international', 'international_scientific', 'scientific_collaboration', 'collaboration_addition', 'addition_actively', 'actively_involve', 'involve_activity', 'activity_contribute', 'contribute_regularly', 'regularly_participate', 'participate_project', 'project_theoretical', 'theoretical_nuclear', 'nuclear_experiment', 'experiment_say', 'say_say', 'say_country', 'country_region', 'region_gain', 'gain_cern']\n",
      "\n",
      "Cleansed:  Berners-Lee worked at the European Organization for Nuclear Research (CERN - it's a French thing). By this time, he had created the first web server, the first web browser, and the first webpage, along with the HyperText Markup Language (HTML) and the HyperText Transfer Protocol (HTTP).\n",
      "\n",
      "Tokenized: ['berners_lee', 'lee_work', 'work_cern', 'cern_thing', 'thing_time', 'time_create', 'create_web', 'web_server', 'server_web', 'web_browser', 'browser_webpage', 'webpage_html']\n",
      "\n",
      "Cleansed: In others, it is impossible to remember life before immediate access, buying living room furniture on your phone and bellyaching about WiFi access. On that April Friday in 1993, CERN, the short name for the European Organization for Nuclear Research, released the codes to the public. It made it available to everyone and offered endless possibilities of technology to us.\n",
      "\n",
      "Tokenized: ['impossible_remember', 'remember_life', 'life_immediate', 'immediate_access', 'access_buy', 'buy_living', 'living_room', 'room_furniture', 'furniture_phone', 'phone_bellyache', 'bellyache_access', 'access_short', 'short_release', 'release_code', 'code_public', 'public_available', 'available_offer', 'offer_endless', 'endless_possibility', 'possibility_technology']\n",
      "\n",
      "Cleansed: Doha   TWO Qatari electrical engineering students, who are graduating from Texas A&M University at Qatar (TAMUQ) this year, have spoken of their life-changing internships with CERN, the largest research center in the world for high energy physics. Qatari brother and sister Abdulaziz al Qahtani and Shaikha al Qahtani spent eight weeks in CERN, the European Organization for Nuclear Research. Joining the day-to-day work of research teams participating in cutting-edge experiments in Geneva, Switzerland, the siblings say they are determined to further engage in research-driven programs.\n",
      "\n",
      "Tokenized: ['electrical_engineering', 'engineering_student', 'student_graduate', 'graduate_tamuq', 'tamuq_speak', 'speak_life', 'life_change', 'change_internship', 'internship_large', 'large_research', 'research_center', 'center_world', 'world_high', 'high_energy', 'energy_physic', 'physic_brother', 'brother_sister', 'sister_spend', 'spend_join', 'join_day', 'day_day', 'day_work', 'work_research', 'research_team', 'team_participate', 'participate_cut', 'cut_edge', 'edge_experiment', 'experiment_sibling', 'sibling_determined', 'determined_engage', 'engage_research', 'research_drive', 'drive_program']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Cleansed: \"+clean_documents[i]+\"\\n\")\n",
    "    print(\"Tokenized: \"+str(tokenized_documents[i])+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea01784",
   "metadata": {},
   "source": [
    "Getting the vectorizers from scikit-learn. scikit-learn vectorizers tokenize documents unless specified other tokenizer, here we already have tokenized documents so we need to provide a \"tokenizer\"-function that returns the tokenized document we want to vectorize, which we already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5d1e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "#the dummy function that returns the already tokenized document\n",
    "def id_fun(already_tokenized):\n",
    "    return already_tokenized\n",
    "\n",
    "#initializing tf-idf\n",
    "if tf_idf_vectorization:\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        tokenizer=id_fun,\n",
    "        preprocessor=id_fun,\n",
    "        token_pattern=None)\n",
    "    \n",
    "#initializing bag-of-words\n",
    "if bow_vectorization:\n",
    "    vectorizer = CountVectorizer(\n",
    "        analyzer='word',\n",
    "        tokenizer=id_fun,\n",
    "        preprocessor=id_fun,\n",
    "        token_pattern=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e56140",
   "metadata": {},
   "source": [
    "Applying the vectorization on the tokenized documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3ba1fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = vectorizer.fit_transform(tokenized_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e920b7f0",
   "metadata": {},
   "source": [
    "The vectorized data is in the form of a sparse matrix. The vocabulary can be retrieved with the '.get_feature_names()'-function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7276b2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "['young_work' 'youngster_interested' 'youth_visit' 'z._jurgen'\n",
      " 'z._renewable' 'zentrum_für' 'zenuity_autonomous' 'zenuity_found'\n",
      " 'zenuity_issue' 'zeplin_draw' 'zip_essentially' 'zip_helium' 'zone_grow'\n",
      " 'zone_launch' 'zoom_electromagnetic' 'zoom_large' '£_sale' 'à_et'\n",
      " 'â(euro)oefor_outstanding' 'â€‹gã‰ant_lead']\n"
     ]
    }
   ],
   "source": [
    "print(type(vectorized))\n",
    "features = vectorizer.get_feature_names_out()\n",
    "print(features[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb9310",
   "metadata": {},
   "source": [
    "<h3>Determining the number of topics or/and clusters to make with the parametric algorithms:<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2982005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d81fc3",
   "metadata": {},
   "source": [
    "<h3>Running Clustering and Topic Modelling Algorithms<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0397df89",
   "metadata": {},
   "source": [
    "Fitting the clustering algorithms. Mean Shift will take long if you have not previously specified to only use a smaller sample of the data. Affinity Propragation will raise an error if it does not converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac767a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lavalton\\AppData\\Local\\anaconda3\\envs\\AI_task\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-means done.\n",
      "Affinity Propagation done. Now patience for Mean Shift.\n",
      "This may take a few hours.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans, AffinityPropagation, MeanShift\n",
    "\n",
    "#Fitting K-means on the vectorized documents:\n",
    "kmeans = KMeans(n_clusters=clusters, random_state=0).fit(vectorized)\n",
    "print(\"K-means done.\")\n",
    "\n",
    "#If Affinity Propagation does not converge, try increasing damping (0,1)\n",
    "ap = AffinityPropagation(affinity='euclidean', damping=0.9, random_state=None).fit(vectorized)\n",
    "print(\"Affinity Propagation done. Now patience for Mean Shift.\\nThis may take a few hours.\")\n",
    "\n",
    "#K-means and Affinity Propagation accept sparce matrices\n",
    "#Mean Shift only array-like, so sparce matrix is transformed to array\n",
    "ms = MeanShift().fit(vectorized.toarray())\n",
    "print(\"Mean Shift done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263143ea",
   "metadata": {},
   "source": [
    "Fitting the topic modelling -algorithms. Gensim algorithms require specific data types (dictionary and corpus), which are specified first. A missing Levenhstein library may raise a Deprecation warning, but similarity-functions are not used here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804fa0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim import matutils, models\n",
    "\n",
    "dictionary = corpora.Dictionary(tokenized_documents)\n",
    "corpus = matutils.Sparse2Corpus(vectorized, documents_columns=False)\n",
    "\n",
    "#Fitting LSI\n",
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=clusters)\n",
    "print(\"LSI done.\")\n",
    "\n",
    "#Fitting LDA\n",
    "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=clusters)\n",
    "print(\"LDA done.\")\n",
    "\n",
    "#Fitting HDP\n",
    "hdp = models.HdpModel(corpus, id2word=dictionary)\n",
    "print(\"HDP done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006d4d90",
   "metadata": {},
   "source": [
    "<h2>Interpretability Analysis<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b6b218",
   "metadata": {},
   "source": [
    "Organizing the tokens of the created clusters according to 'importance':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b359da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-means and Mean Shift return cluster centres as numpy arrays\n",
    "#Affinity Propagation as a sparce matrix that also needs to be\n",
    "#transformed into array form\n",
    "km_order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "ap_order_centroids = ap.cluster_centers_.toarray().argsort()[:, ::-1]\n",
    "ms_order_centroids = ms.cluster_centers_.argsort()[:, ::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8748b660",
   "metadata": {},
   "source": [
    "The function returns a dictionary for a clustering output that assigns a list to top tokens per cluster yielded by the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223c7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clustering(order_centroids, name, top=10):\n",
    "    #Initialising data structure\n",
    "    dictionary = {}\n",
    "    #Iterating through every cluster\n",
    "    for i in range(len(order_centroids)):\n",
    "        ind = i + 1\n",
    "        #Naming cluster\n",
    "        cluster_name = name + \" \" + str(ind)\n",
    "        #Finding top tokens in the ordered list of tokens\n",
    "        token_list = [features[ind] for ind in order_centroids[i, :top]]\n",
    "        #Adding the result to the dictionary\n",
    "        dictionary[cluster_name] = token_list\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f071de72",
   "metadata": {},
   "source": [
    "Creating dictionaries per clustering output for interpretability assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b49329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "km_cluster_dict = print_clustering(km_order_centroids, \"K-means\")\n",
    "ap_cluster_dict = print_clustering(ap_order_centroids, \"Affinity Propagation\")\n",
    "ms_cluster_dict = print_clustering(ms_order_centroids, \"Mean Shift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc73662c",
   "metadata": {},
   "source": [
    "Creating the pandas dataframes from the dictionaries per clustering output for interpretability assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_df = pd.DataFrame.from_dict(km_cluster_dict, orient='index')\n",
    "ap_df = pd.DataFrame.from_dict(ap_cluster_dict, orient='index')\n",
    "ms_df = pd.DataFrame.from_dict(ms_cluster_dict, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d494b85",
   "metadata": {},
   "source": [
    "The function returns a dictionary for a topic modelling output that assigns a list to top tokens per topic yielded by the method. HDP output is slightly different from LSI and LDA, so it is separated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76dd9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, name, clusters, hdp=False, vocab=None, top=10):\n",
    "    #Initialising data structure\n",
    "    dictionary = {}\n",
    "    if not hdp:\n",
    "        topics = model.show_topics(num_topics=clusters, num_words=top)\n",
    "        #Iterating over every topic\n",
    "        for item in topics:\n",
    "            topic = list(item)\n",
    "            #Naming topic\n",
    "            topic_name = name + \" \" + str(int(topic[0])+1)\n",
    "            clean_str = topic[1].replace('\"\"','\"')\n",
    "            terms = re.findall('\\\"(.*?)\\\"', clean_str)\n",
    "            #Finding top tokens in the list of tokens\n",
    "            token_list = [str(token) for token in terms[:top]]\n",
    "            for tok in token_list:\n",
    "                if \"*\" in tok:\n",
    "                    print(token_list)\n",
    "                    print(item)\n",
    "            #Adding the result to the dictionary\n",
    "            dictionary[topic_name] = token_list\n",
    "    else:\n",
    "        num_topics=model.get_topics().shape[0] #HDP creates always 150 topics\n",
    "        topics = model.show_topics(num_topics=num_topics,num_words=top)\n",
    "        #Iterating over every topic\n",
    "        for item in topics:\n",
    "            topic = list(item)\n",
    "            #Naming topic\n",
    "            topic_name = name + \" \" + str(int(topic[0])+1)\n",
    "            #Cleansing the output to find tokens\n",
    "            tokens_to_clean = [token for token in topic[1].split(\"+\")]\n",
    "            to_clean_2 = [token.split(\"*\")[1] for token in tokens_to_clean]\n",
    "            tokens_clean = [clean.strip() for clean in [token.split(\"*\")[1] for token in tokens_to_clean]]\n",
    "            #Finding top tokens in the cleansed list of tokens\n",
    "            token_list = [str(token) for token in tokens_clean[:-1]]\n",
    "            #Adding the result to the dictionary\n",
    "            dictionary[topic_name] = token_list\n",
    "    return dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e2f2a",
   "metadata": {},
   "source": [
    "Creating dictionaries per topic modelling output for interpretability assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f28910",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_topic_dict = print_topics(lsi, \"LSI\", clusters)\n",
    "lda_topic_dict = print_topics(lda, \"LDA\", clusters)\n",
    "hdp_topic_dict = print_topics(hdp, \"HDP\", clusters, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f6f7c1",
   "metadata": {},
   "source": [
    "Creating the pandas dataframes from the dictionaries per topic modelling output for interpretability assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e4221",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_df = pd.DataFrame.from_dict(lsi_topic_dict, orient='index')\n",
    "lda_df = pd.DataFrame.from_dict(lda_topic_dict, orient='index')\n",
    "hdp_df = pd.DataFrame.from_dict(hdp_topic_dict, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09e5628",
   "metadata": {},
   "source": [
    "Combining the topic modelling and clustering interpretability dataframes to an Excel-file, algorithm per sheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7893b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"interpretability_assessment_\"+str(\"_\".join(pre_processing_name()))+\".xlsx\") as writer:  \n",
    "    kmeans_df.to_excel(writer, sheet_name='K-means')\n",
    "    ap_df.to_excel(writer, sheet_name='Aff. Prop.')\n",
    "    ms_df.to_excel(writer, sheet_name='Mean Shift')\n",
    "    lda_df.to_excel(writer, sheet_name='LDA')\n",
    "    lsi_df.to_excel(writer, sheet_name='LSI')\n",
    "    hdp_df.to_excel(writer, sheet_name='HDP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb585a",
   "metadata": {},
   "source": [
    "<h2>Representativeness Analysis<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf03409",
   "metadata": {},
   "source": [
    "Next we find for each text document in the vectorized data form which cluster it belongs to with the inbuilt \"predict\" functions of each clustering algorithm, and add these predictions to the dataframe along with the corresponding top tokens of the assigned cluster. We have chosen to use the predict-function, since then this section of the notebook can be used to predict new, unseen documents if so wished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aed73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the K-means clusters for the vectorized documents\n",
    "km_pred = kmeans.predict(vectorized)\n",
    "#Getting the cluster names for the predictions\n",
    "km_clusters = [\"K-means \"+str(cluster+1) for cluster in km_pred]\n",
    "#Getting the cluster tokens per cluster prediction\n",
    "km_cluster_tokens = [km_cluster_dict[cluster] for cluster in km_clusters]\n",
    "\n",
    "#Inserting the predictions and tokens into the dataframe as columns\n",
    "df['K-means Clusters'] = km_clusters\n",
    "df['K-means tokens'] = [\"; \".join(token_list) for token_list in km_cluster_tokens]\n",
    "\n",
    "#AP and MS predict attributes accept only an array-like data\n",
    "\n",
    "#Predicting the Affinity Propagation clusters for the vectorized documents\n",
    "ap_pred = ap.predict(vectorized.toarray())\n",
    "#Getting the cluster names for the predictions\n",
    "ap_clusters = [\"Affinity Propagation \"+str(cluster+1) for cluster in ap_pred]\n",
    "#Getting the cluster tokens per cluster prediction\n",
    "ap_cluster_tokens = [ap_cluster_dict[cluster] for cluster in ap_clusters]\n",
    "\n",
    "#Inserting the predictions and tokens into the dataframe as columns\n",
    "df['Affinity Propagation Clusters'] = ap_clusters\n",
    "df['Affinity Propragation tokens'] = [\"; \".join(token_list) for token_list in ap_cluster_tokens]\n",
    "\n",
    "#Predicting the Mean Shift clusters for the vectorized documents\n",
    "ms_pred = ms.predict(vectorized.toarray())\n",
    "#Getting the cluster names for the predictions\n",
    "ms_clusters = [\"Mean Shift \"+str(cluster+1) for cluster in ms_pred]\n",
    "#Getting the cluster tokens per cluster prediction\n",
    "ms_cluster_tokens = [ms_cluster_dict[cluster] for cluster in ms_clusters]                   \n",
    "\n",
    "#Inserting the predictions and tokens into the dataframe as columns\n",
    "df['Mean Shift Clusters'] = ms_clusters\n",
    "df['Mean Shift tokens'] = [\"; \".join(token_list) for token_list in ms_cluster_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a86f582",
   "metadata": {},
   "source": [
    "This function sorts a tuple. It is used to find the topic model with the highest probability of each topic per document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b99e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_tuple(tup):   \n",
    "    tup.sort(key = lambda x: abs(x[1]), reverse=True)  \n",
    "    return tup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843e66da",
   "metadata": {},
   "source": [
    "Initializing the same lists for topic modelling as were just now made for clustering: The predicted top topic and the tokens that go along with that topic. Here there are multiple topics a document belongs to due to the probabilistic nature of topic modelling, but we look at only the most representative one. We record the probabilities of the most representative topics as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb201e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics = []\n",
    "lda_probabilities = []\n",
    "lda_topic_tokens = []\n",
    "\n",
    "lsi_topics = []\n",
    "lsi_probabilities = []\n",
    "lsi_topic_tokens = []\n",
    "\n",
    "hdp_topics = []\n",
    "hdp_probabilities = []\n",
    "hdp_topic_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ee55a",
   "metadata": {},
   "source": [
    "Next the corpus - created by gensim for topic modelling - is iterated and each document is predicted a topic distribution similarly to clustering. From the result, the top representative topic, its percentage/probability, and its topics are collected and appended to the lists initialized previously. If some documents do not have any topics assigned to them, they are treated with exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2262c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in corpus:\n",
    "    #LDA\n",
    "    #Get ordered topic distribution for document\n",
    "    lda_res = sort_tuple(lda[item])\n",
    "    if lda_res:\n",
    "        #Get topic and name it\n",
    "        lda_topic = \"LDA \" +str(lda_res[0][0]+1)\n",
    "        #Add to list the topic name\n",
    "        lda_topics.append(lda_topic)\n",
    "        #Add to list the tokens of topic\n",
    "        lda_topic_tokens.append(lda_topic_dict[lda_topic])\n",
    "        #Add to list the probability of topic\n",
    "        lda_probabilities.append(lda_res[0][1])\n",
    "    else:\n",
    "        #If no result, assign missing or NaN values instead\n",
    "        lda_topic = \"LDA NaN\"\n",
    "        lda_topics.append(lda_topic)\n",
    "        lda_topic_tokens.append([\"Missing\"])\n",
    "        lda_probabilities.append(\"LDA NaN\")          \n",
    "\n",
    "    #LSI\n",
    "    #Get ordered topic distribution for document\n",
    "    lsi_res = sort_tuple(lsi[item])\n",
    "    if lsi_res:\n",
    "        #Get topic and name it\n",
    "        lsi_topic = \"LSI \"+str(lsi_res[0][0]+1)\n",
    "        #Add to list the topic name\n",
    "        lsi_topics.append(lsi_topic)\n",
    "        #Add to list the tokens of topic\n",
    "        lsi_topic_tokens.append(lsi_topic_dict[lsi_topic])\n",
    "        #Add to list the probability of topic\n",
    "        lsi_probabilities.append(lsi_res[0][1])\n",
    "    else:\n",
    "        #If no result, assign missing or NaN values instead\n",
    "        lsi_topic = \"LSI NaN\"\n",
    "        lsi_topics.append(lsi_topic)\n",
    "        lsi_topic_tokens.append([\"Missing\"])\n",
    "        lsi_probabilities.append(\"LSI NaN\")           \n",
    "\n",
    "    #HDP\n",
    "    #Get ordered topic distribution for document\n",
    "    hdp_res = sort_tuple(hdp[item])\n",
    "    if hdp_res:\n",
    "        #Get topic and name it\n",
    "        hdp_topic = \"HDP \" +str(hdp_res[0][0]+1)\n",
    "        #Add to list the topic name\n",
    "        hdp_topics.append(hdp_topic)\n",
    "        #Add to list the tokens of topic\n",
    "        hdp_topic_tokens.append(hdp_topic_dict[hdp_topic])\n",
    "        #Add to list the probability of topic\n",
    "        hdp_probabilities.append(hdp_res[0][1])\n",
    "    else:\n",
    "        #If no result, assign missing or NaN values instead\n",
    "        hdp_topic = \"HDP NaN\"\n",
    "        hdp_topics.append(hdp_topic)\n",
    "        hdp_topic_tokens.append([\"Missing\"])\n",
    "        hdp_probabilities.append(\"HDP NaN\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b02977",
   "metadata": {},
   "source": [
    "Adding the topic modelling topics, probabilities, and tokens as columns to the dataframe. Token lists are formatted to separate tokens in list with \";\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46d0350",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7577dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LDA Topic']=lda_topics\n",
    "df['LDA Probability']=lda_probabilities\n",
    "df['LDA Topic Tokens'] = [\"; \".join(token_list) for token_list in lda_topic_tokens]\n",
    "\n",
    "df['LSI Topic']=lsi_topics\n",
    "df['LSI Probability']=lsi_probabilities\n",
    "df['LSI Topic Tokens'] = [\"; \".join(token_list) for token_list in lsi_topic_tokens]\n",
    "\n",
    "df['HDP Topic']=hdp_topics\n",
    "df['HDP Probability']=hdp_probabilities\n",
    "df['HDP Topic Tokens'] = [\"; \".join(token_list) for token_list in hdp_topic_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed24976",
   "metadata": {},
   "source": [
    "Formatting dataframes of the representativeness data to be used as sheets in the Excel-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb59026",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmdf = df[['Document', 'Label','Action Clean','Tokenized','K-means Clusters','K-means tokens']]\n",
    "apdf = df[['Document', 'Label','Action Clean','Tokenized','Affinity Propagation Clusters','Affinity Propragation tokens']]\n",
    "msdf = df[['Document', 'Label','Action Clean','Tokenized','Mean Shift Clusters','Mean Shift tokens']]\n",
    "ldadf = df[['Document', 'Label','Action Clean','Tokenized','LDA Topic','LDA Probability','LDA Topic Tokens']]\n",
    "lsidf = df[['Document', 'Label','Action Clean','Tokenized','LSI Topic','LSI Probability','LSI Topic Tokens']]\n",
    "hdpdf = df[['Document', 'Label','Action Clean','Tokenized','HDP Topic','HDP Probability','HDP Topic Tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a78940",
   "metadata": {},
   "source": [
    "Creating the Excel-file from the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"representativeness_assessment_\"+str(\"_\".join(pre_processing_name()))+\".xlsx\") as writer:  \n",
    "    kmdf.to_excel(writer, sheet_name='K-means')\n",
    "    apdf.to_excel(writer, sheet_name='Aff. Prop.')\n",
    "    msdf.to_excel(writer, sheet_name='Mean Shift')\n",
    "    ldadf.to_excel(writer, sheet_name='LDA')\n",
    "    lsidf.to_excel(writer, sheet_name='LSI')\n",
    "    hdpdf.to_excel(writer, sheet_name='HDP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603bd388",
   "metadata": {},
   "source": [
    "<h3>Confusion Matrices and Comparison to SML<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a617af",
   "metadata": {},
   "source": [
    "The used text documents were also coded by a human into seven predefined categories: Informational, Human, Organizational, Relational, Financial, Legal, and Physical. Here we set LDA and K-means UML to create also 7 clusters or topics, and we compare the clusters and topics against the human categorization with a confusion matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026e93ce",
   "metadata": {},
   "source": [
    "Setting the number of clusters/topics to make to match the SML categorization and creating a dictionary to abbreviate the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7c8db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_sml_classes = 3\n",
    "category_dict = {\"Human capital\":\"Hu\",\"Technology\":\"Tech\",\"Scientific knowledge\":\"Sci\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb0a177",
   "metadata": {},
   "source": [
    "While we would absolutely love to make beautiful graphics for you, most libraries to do this create conflicts with the current ones - at least for now. So alas, we need to make due with simple text representations of the results. Also we show how many documents are in each topic/cluster/SML category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0cd0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "\n",
    "#Creating the LDA model with 7 topics\n",
    "confusion_lda = models.LdaModel(corpus, id2word=dictionary, num_topics=number_of_sml_classes)\n",
    "#Creating the K-means model with 7 clusters\n",
    "confusion_kmeans = KMeans(n_clusters=number_of_sml_classes, random_state=0).fit(vectorized)\n",
    "\n",
    "#Abberivating the categories and adding the abbreviations to the dataframe\n",
    "df['Class ID'] = [category_dict[item] for item in list(df['Label'])]\n",
    "#Finding the most representative topics for each document in corpus\n",
    "df['LDA Confusion analysis'] = [sort_tuple(confusion_lda[item])[0][0]+1 for item in corpus]\n",
    "#Finding the predicted clusters for each document in documents (same as corpus but different format)\n",
    "df['K-means Confusion analysis'] = [str(cluster+1) for cluster in confusion_kmeans.predict(vectorized)]\n",
    "\n",
    "#The confusion matrix of LDA against SML categorization\n",
    "print(\"LDA\")\n",
    "lda_c_confusion_matrix = pd.crosstab(df['Class ID'], df['LDA Confusion analysis'], \n",
    "                                     rownames=['SML'], colnames=['Topic'])\n",
    "print(lda_c_confusion_matrix)\n",
    "\n",
    "print(\"\\nNumber of documents in each topic: \")\n",
    "print(collections.Counter(list(df['LDA Confusion analysis'])).most_common(7))\n",
    "print(\"Number of documents in each SML category: \")\n",
    "print(collections.Counter(list(df['Class ID'])).most_common(7))\n",
    "\n",
    "#The confusion matrix of K-means against SML categorization\n",
    "print(\"\\nK-means\")\n",
    "km_confusion_matrix = pd.crosstab(df['Class ID'], df['K-means Confusion analysis'], \n",
    "                                  rownames=['SML'], colnames=['Topic'])\n",
    "print(km_confusion_matrix)\n",
    "\n",
    "print(\"\\nNumber of documents in each cluster: \")\n",
    "print(collections.Counter(list(df['K-means Confusion analysis'])).most_common(7))\n",
    "print(\"Number of documents in each SML category: \")\n",
    "print(collections.Counter(list(df['Class ID'])).most_common(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4052ed",
   "metadata": {},
   "source": [
    "Depending on the pre-processing, but usually you will find that topic modelling topics are dispersed all over the matrices, while clustering is more sparce and concentrated. This demonstrates the probabilistic nature of topic modelling: \"All topics can be found in all documents\", making it difficult to assign a document to a topic or a topic to a document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccbbf52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
