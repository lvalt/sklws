{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "824355aa",
   "metadata": {},
   "source": [
    "## Below type in the specifications for how you want to run the example with CERN news pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e88db70",
   "metadata": {},
   "source": [
    "For language to be understood by computers, it needs to be turned into a numerical form. That begins with separating the words in a document into units called tokens. Here, the example uses simple tokens of singular words which are lemmatized: For the words the \"base\" form is found out with a statistical approach.\n",
    "\n",
    "<b>There are different ways to do tokenization (e.g., n-grams) and lemmatization (e.g., stemming or doing nothing). Quickly search for the benefits of the different approaches and decide what you want to choose for the few of such selected options for parameters below.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2607115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Choose below \"NO\" for no stopword removal, \"YES\" for stopword removal\n",
    "'''\n",
    "chosen_stopword_removal = \"YES\"\n",
    "\n",
    "'''\n",
    "Choose below \"YES\" to lemmatize documents, else choose \"NO\"\n",
    "'''\n",
    "chosen_lemmatization = \"YES\"\n",
    "\n",
    "'''\n",
    "Choose below \"BOW\" or \"TFIDF\" for vectorization\n",
    "'''\n",
    "chosen_vectorization = \"TFIDF\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd46173c",
   "metadata": {},
   "source": [
    "Below are some different types of machine learning algorithms. Acquaint yourself with the basic pros and cons of each. If you wish to, you can also find other types of ML to learn about at scikit-learn.org.\n",
    "\n",
    "__[Neural Network](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)__\n",
    "\n",
    "__[Support Vector Machine](https://scikit-learn.org/stable/modules/svm.html#svm)__\n",
    "\n",
    "__[Nearest Neighbours](https://scikit-learn.org/stable/modules/neighbors.html)__\n",
    "\n",
    "__[Decision Tree](https://scikit-learn.org/stable/modules/tree.html#tree)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b214ce",
   "metadata": {},
   "source": [
    "Some specifics are required to the algorithms such as how many iterations to use, how to scale data, etc. If you want to get acquainted with the different such options, use the links above to learn about the parameters and their effects.\n",
    "\n",
    "However, <b>consider: Who would know which parameters to use and why if you were to implement AI in your organization?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e8c7591",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Choose below\n",
    "\"KNN\" for nearest neighbours\n",
    "\"NN\" for neural network\n",
    "\"SVM\" for support vector machine\n",
    "\"Tree\" for decision-tree\n",
    "'''\n",
    "chosen_algorithm = \"NN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3adf6",
   "metadata": {},
   "source": [
    "Here we import the basic libraries required for data analysis with Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "602e1836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we import the basic Python data analytics libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd   # includes tools used in reading data\n",
    "import numpy as np   # includes tools for numerical calculus\n",
    "import matplotlib.pyplot as plt  # includes tools used in plotting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c63c99",
   "metadata": {},
   "source": [
    "Below we define a function that we can use to track how much time and memory the code uses between a certain part of running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a36c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import time\n",
    "\n",
    "def track_memory_and_time(start_time=None):\n",
    "    \"\"\"\n",
    "    Track memory usage and elapsed time since a specific point in the code.\n",
    "\n",
    "    Parameters:\n",
    "    - start_time: Optional parameter. If provided, it should be the result of a previous call to time.time().\n",
    "\n",
    "    Returns:\n",
    "    - memory_usage: Current memory usage in bytes.\n",
    "    - elapsed_time: Elapsed time in seconds since the specified start time.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get memory usage\n",
    "    memory_usage = psutil.virtual_memory().used\n",
    "\n",
    "    # Get elapsed time\n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - start_time if start_time else 0\n",
    "\n",
    "    return memory_usage, elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "878668dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining and creating a function with which you can later on get the accuracy of the ML/AI algorithm\n",
    "def accuracy(clf, X_test, y_test):\n",
    "    score = str(clf.score(X_test, y_test)*100)+'%'\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d82ec9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is just defining and creating a function with which you can later on compare the AI predictions to the original data\n",
    "def find_differences(clf, X_test, y_test):\n",
    "    \n",
    "    predictions = []\n",
    "    test_labels = []\n",
    "    column_names = list(X_test.columns.values)\n",
    "    events = pd.DataFrame(columns=list(X_test.columns.values))\n",
    "    \n",
    "    for i in range(len(list(y_test))-1):\n",
    "        if str(clf.predict([X_test.values[i+1]])[0]) != list(y_test)[i+1]:\n",
    "            predictions.append(clf.predict([X_test.values[i+1]])[0])\n",
    "            test_labels.append(list(y_test)[i+1])\n",
    "            events.loc[len(events)] = X_test.values[i+1]\n",
    "               \n",
    "    df = pd.DataFrame(columns=['Prediction', 'Original label'])\n",
    "    df['Prediction']= predictions\n",
    "    df['Original label']= test_labels\n",
    "\n",
    "    df = pd.concat([df, events], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d2956df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move this block to the location in the code from where you want to start tracking\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ac8e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we import the different ML algorithms from the scikit-learn library\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#Neural Network\n",
    "\n",
    "from sklearn import svm\n",
    "#Support Vector Machine\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier \n",
    "#Nearest Neighbours\n",
    "\n",
    "from sklearn import tree\n",
    "#Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba131392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_algorithm(name):\n",
    "    if name == \"NN\":\n",
    "        clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
    "    elif name == \"KNN\":\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        clf = Pipeline(steps=[(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier(n_neighbors=11))])\n",
    "    elif name == \"SVM\":\n",
    "        clf = svm.SVC(kernel='rbf')#'linear' or 'rbf'\n",
    "    else:\n",
    "        clf = tree.DecisionTreeClassifier(max_depth=12)\n",
    "        \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb86dd",
   "metadata": {},
   "source": [
    "## Qualitative\n",
    "\n",
    "For supervised machine learning, you need a dataset and correct labels to give to it.\n",
    "\n",
    "<b>For your work, think of an AI task using qualitative data, e.g., text of images, that would be useful for you and determine the type of data you would need to teach to an AI: Where or how can you get 10k+ examples with the correct \"labels\" assigned to the datapoints - what resources would you need to create/retrieve this data?</b>\n",
    "\n",
    "The example here uses text pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0463c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46632a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 435.7 kB/s eta 0:00:30\n",
      "     --------------------------------------- 0.1/12.8 MB 762.6 kB/s eta 0:00:17\n",
      "     - -------------------------------------- 0.5/12.8 MB 3.6 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 8.1 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.7/12.8 MB 11.4 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 3.9/12.8 MB 13.8 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 4.8/12.8 MB 14.6 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 13.8 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.1/12.8 MB 14.4 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 7.0/12.8 MB 14.9 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.8/12.8 MB 15.1 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 15.8 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 10.0/12.8 MB 16.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 10.8/12.8 MB 19.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.1/12.8 MB 20.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 20.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 19.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lavalton\\appdata\\local\\anaconda3\\envs\\ai_task\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e20abc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Importing the Python libraries to analyze and treat text data.\n",
    "'''\n",
    "import re\n",
    "import spacy\n",
    "spacy_model = \"en_core_web_sm\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349a67dd",
   "metadata": {},
   "source": [
    "Opening up the file with the data, this is the example, if you have different data in mind, you can use that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83b70485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since starting at KU she has worked with Disti...</td>\n",
       "      <td>Human capital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inc. All Rights Reserved\\n\\n\\nLength: 260 word...</td>\n",
       "      <td>Human capital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Founded in 2004, Zecotek operates three divisi...</td>\n",
       "      <td>Human capital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Winning Company Details: The European Organiza...</td>\n",
       "      <td>Scientific knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Page  of \\nCERN exhibition due to begin in Kuw...</td>\n",
       "      <td>Human capital</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document                 Label\n",
       "0  Since starting at KU she has worked with Disti...         Human capital\n",
       "1  Inc. All Rights Reserved\\n\\n\\nLength: 260 word...         Human capital\n",
       "2  Founded in 2004, Zecotek operates three divisi...         Human capital\n",
       "3  Winning Company Details: The European Organiza...  Scientific knowledge\n",
       "4  Page  of \\nCERN exhibition due to begin in Kuw...         Human capital"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('cern_news_data.xlsx')\n",
    "#Choosing the columns we want for our analysis\n",
    "df = df[['Document', 'Label']]\n",
    "#Dropping rows with empty data\n",
    "df = df.dropna(how = 'any',axis = 0).reset_index(drop = True)\n",
    "#Dropping rows of duplicate text documents\n",
    "df = df.drop_duplicates(subset=\"Document\")\n",
    "df = df.sample(frac = 1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49311101",
   "metadata": {},
   "source": [
    "<b>What operations do you need to perform on the data you have to clean it up?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e877528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a new list of text documents with extra whitespace and asterisks and quotation marks \n",
    "#that would complicate further cleansing removed\n",
    "cleaner_documents = [text.replace(\"*\", \" \").replace('\"','') for text in list(df['Document'])]\n",
    "clean_documents = [re.sub('[\\s+]', ' ',text) for text in cleaner_documents]\n",
    "\n",
    "#Adding the cleansed documents to the dataframe\n",
    "df['Document Clean'] = clean_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afa2600e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since starting at KU she has worked with Distinguished Professor of Physics and Astronomy Alice Bean and Professor of Physics and Astronomy Phil Baringer in the Particle Physics Laboratory. With them, she has been involved in research at CERN (The European Organization for Nuclear Research) in Geneva, Switzerland. Her current research focuses on the search for the theoretical top-prime particle in relation to the recently-discovered Higgs boson.\n",
      "\n",
      "Inc. All Rights Reserved   Length: 260 words Body   Limelight Networks reported that the European Organization for Nuclear Research (CERN) is using Limelight's Content Delivery Network (CDN) to live stream scientific discoveries to thousands of physicists and engineers worldwide. CERN, a physics laboratory, regularly shares knowledge with scientists and the general public around the world.\n",
      "\n",
      "Founded in 2004, Zecotek operates three divisions: Imaging Systems, Optronics Systems and 3D Display Systems with labs located in Canada, Korea, Russia, Singapore and U.S.A. The management team is focused on building shareholder value by commercializing over 50 patented and patent pending novel photonic technologies directly and through strategic alliances and joint ventures with leading industry partners including Hamamatsu Photonics (Japan), the European Organization for Nuclear Research (Switzerland), Beijing Opto-Electronics Technology Co. Ltd. (China), NuCare Medical Systems (South Korea), the University ofWashington (United States), and National NanoFab Center (South Korea).\n",
      "\n",
      "Winning Company Details: The European Organization for Nuclear Research (CERN)  Geneva 23 \n",
      "\n",
      "Page  of  CERN exhibition due to begin in Kuwait Monday Page  of  European Organization for Nuclear Research Council Appoints Fabiola Gianotti for Second Term of Office as CERN Director General Page  of\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Seeing what the cleansed documents look like\n",
    "for clean_document in clean_documents[:5]:\n",
    "    print(clean_document+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9f46e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleansed: Since starting at KU she has worked with Distinguished Professor of Physics and Astronomy Alice Bean and Professor of Physics and Astronomy Phil Baringer in the Particle Physics Laboratory. With them, she has been involved in research at CERN (The European Organization for Nuclear Research) in Geneva, Switzerland. Her current research focuses on the search for the theoretical top-prime particle in relation to the recently-discovered Higgs boson.\n",
      "\n",
      "Tokenized: ['start', 'ku', 'work', 'professor', 'physics', 'involve', 'research', 'current', 'research', 'focus', 'search', 'theoretical', 'prime', 'particle', 'relation', 'recently', 'discover']\n",
      "\n",
      "Cleansed: Inc. All Rights Reserved   Length: 260 words Body   Limelight Networks reported that the European Organization for Nuclear Research (CERN) is using Limelight's Content Delivery Network (CDN) to live stream scientific discoveries to thousands of physicists and engineers worldwide. CERN, a physics laboratory, regularly shares knowledge with scientists and the general public around the world.\n",
      "\n",
      "Tokenized: ['inc.', 'word', 'body', 'report', 'cern', 'live', 'stream', 'scientific', 'discovery', 'physicist', 'engineer', 'worldwide', 'physics', 'laboratory', 'regularly', 'share', 'knowledge', 'scientist', 'general', 'public', 'world']\n",
      "\n",
      "Cleansed: Founded in 2004, Zecotek operates three divisions: Imaging Systems, Optronics Systems and 3D Display Systems with labs located in Canada, Korea, Russia, Singapore and U.S.A. The management team is focused on building shareholder value by commercializing over 50 patented and patent pending novel photonic technologies directly and through strategic alliances and joint ventures with leading industry partners including Hamamatsu Photonics (Japan), the European Organization for Nuclear Research (Switzerland), Beijing Opto-Electronics Technology Co. Ltd. (China), NuCare Medical Systems (South Korea), the University ofWashington (United States), and National NanoFab Center (South Korea).\n",
      "\n",
      "Tokenized: ['found', 'operate', 'division', 'lab', 'locate', 'management', 'team', 'focus', 'build', 'shareholder', 'value', 'commercialize', 'patented', 'patent', 'pende', 'novel', 'photonic', 'technology', 'directly', 'strategic', 'alliance', 'joint', 'venture', 'lead', 'industry', 'partner', 'include']\n",
      "\n",
      "Cleansed: Winning Company Details: The European Organization for Nuclear Research (CERN)  Geneva 23 \n",
      "\n",
      "Tokenized: ['win', 'company', 'detail', 'cern']\n",
      "\n",
      "Cleansed: Page  of  CERN exhibition due to begin in Kuwait Monday Page  of  European Organization for Nuclear Research Council Appoints Fabiola Gianotti for Second Term of Office as CERN Director General Page  of\n",
      "\n",
      "Tokenized: ['page', 'exhibition', 'begin', 'page', 'appoints', 'director', 'general']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization:\n",
    "lemmatized = True\n",
    "if chosen_lemmatization == \"NO\":\n",
    "    lemmatized = False\n",
    "\n",
    "#Loading a python library for natural language processing\n",
    "nlp = spacy.load(spacy_model, disable=['merge_noun_chunks'])\n",
    "\n",
    "#Creating a function that will dothe basic tokenization of the documents\n",
    "def basic_tokenizer(document, lemmatized=lemmatized):\n",
    "    #Converting the text document into a Spacy document\n",
    "    document = nlp(document)\n",
    "    if chosen_stopword_removal == \"YES\":\n",
    "        if not lemmatized:\n",
    "            tokenized = [token.text.lower() for token in document if token.ent_iob == 2 #<- This removes known entities\n",
    "                         and not (token.is_stop or token.is_punct or token.is_space or token.like_num \n",
    "                                  or token.like_url or token.like_email)]\n",
    "        if lemmatized:\n",
    "            tokenized = [token.lemma_.lower() for token in document if token.ent_iob == 2 #<- This removes known entities\n",
    "                         and not (token.is_stop or token.is_punct or token.is_space or token.like_num \n",
    "                                  or token.like_url or token.like_email)]\n",
    "    else:\n",
    "        if not lemmatized:\n",
    "            tokenized = [token.text.lower() for token in document if token.ent_iob == 2 #<- This removes known entities\n",
    "                         and not (token.is_punct or token.is_space or token.like_num \n",
    "                                  or token.like_url or token.like_email)]\n",
    "        if lemmatized:\n",
    "            tokenized = [token.lemma_.lower() for token in document if token.ent_iob == 2 #<- This removes known entities\n",
    "                         and not (token.is_punct or token.is_space or token.like_num \n",
    "                                  or token.like_url or token.like_email)]        \n",
    "    #Returns a list of tokens\n",
    "    return tokenized\n",
    "\n",
    "#Initializing a list where to add the treated documents\n",
    "tokenized_documents = []\n",
    "\n",
    "for document in clean_documents:\n",
    "    #using basic tokenizer on the document with Spacy's chunks disabled\n",
    "    tokenized_documents.append(basic_tokenizer(document))\n",
    "\n",
    "#Adding the tokenized documents to the dataframe\n",
    "df['Tokenized'] = tokenized_documents\n",
    "\n",
    "#printing a few examples of what the treated documents look like now\n",
    "for i in range(5):\n",
    "    print(\"Cleansed: \"+clean_documents[i]+\"\\n\")\n",
    "    print(\"Tokenized: \"+str(tokenized_documents[i])+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75492f35",
   "metadata": {},
   "source": [
    "After the documents have been tokenized and treated as still words, it's relevant to turn the text into numeric form that computer algorithms can understand. Two common ways are TFIDF and BOW vectorizations. Acquaint yourself with both, and choose which one you want to use.\n",
    "\n",
    "Do you want to use __[TFIDF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#)__ or __[Bag-of-Words](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)__ vectorization?\n",
    "\n",
    "<b>Explain why you decided as you did</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3952795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "#the dummy function that returns the already tokenized document\n",
    "def id_fun(already_tokenized):\n",
    "    return already_tokenized\n",
    "\n",
    "#initializing tf-idf\n",
    "if chosen_vectorization == \"TFIDF\":\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        tokenizer=id_fun,\n",
    "        preprocessor=id_fun,\n",
    "        token_pattern=None)\n",
    "    \n",
    "#initializing bag-of-words\n",
    "else:\n",
    "    vectorizer = CountVectorizer(\n",
    "        analyzer='word',\n",
    "        tokenizer=id_fun,\n",
    "        preprocessor=id_fun,\n",
    "        token_pattern=None)\n",
    "    \n",
    "#implementing the vectorization\n",
    "vectorized = vectorizer.fit_transform(tokenized_documents)\n",
    "#tweaking the form of the data for analysis\n",
    "dense = vectorized.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10b6390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Making a simple function that will name all the columns \n",
    "in the dataset of the vectorized documents\n",
    "'''\n",
    "\n",
    "def name_x(dense, doc):\n",
    "    shape = dense.shape\n",
    "    # Generate column names with running numeration\n",
    "    column_names = [f'x_{i+1}' for i in range(shape[1])]\n",
    "\n",
    "    data = dense\n",
    "\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "    df['Clean'] = doc\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e0bb135",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>x_10</th>\n",
       "      <th>...</th>\n",
       "      <th>x_4092</th>\n",
       "      <th>x_4093</th>\n",
       "      <th>x_4094</th>\n",
       "      <th>x_4095</th>\n",
       "      <th>x_4096</th>\n",
       "      <th>x_4097</th>\n",
       "      <th>x_4098</th>\n",
       "      <th>x_4099</th>\n",
       "      <th>x_4100</th>\n",
       "      <th>Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Since starting at KU she has worked with Disti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Inc. All Rights Reserved   Length: 260 words B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Founded in 2004, Zecotek operates three divisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Winning Company Details: The European Organiza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Page  of  CERN exhibition due to begin in Kuwa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 4101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   x_1  x_2  x_3  x_4  x_5  x_6  x_7  x_8  x_9  x_10  ...  x_4092  x_4093  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...     0.0     0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...     0.0     0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...     0.0     0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...     0.0     0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...     0.0     0.0   \n",
       "\n",
       "   x_4094  x_4095  x_4096  x_4097  x_4098  x_4099  x_4100  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "                                               Clean  \n",
       "0  Since starting at KU she has worked with Disti...  \n",
       "1  Inc. All Rights Reserved   Length: 260 words B...  \n",
       "2  Founded in 2004, Zecotek operates three divisi...  \n",
       "3  Winning Company Details: The European Organiza...  \n",
       "4  Page  of  CERN exhibition due to begin in Kuwa...  \n",
       "\n",
       "[5 rows x 4101 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Defining which part of the data is the data and which one is the label,\n",
    "using the function defined just previously\n",
    "'''\n",
    "\n",
    "X = name_x(dense, df['Document Clean'])\n",
    "y =  df['Label']\n",
    "\n",
    "#printing out a small example of what the data looks like\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2621647",
   "metadata": {},
   "source": [
    "<b>What are the cleaning operations performed above on the CERN datasets? Try to search for what they do and think about why they should/should not be performed.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed187be",
   "metadata": {},
   "source": [
    "Then we split the found data to training and testing data. \n",
    "Sometimes we also use validation data. \n",
    "\n",
    "<b>What are common splits in percentages</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3108069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_Train, X_Test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a663a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the non-numerical column from the data before passing this on to the ML algorithms\n",
    "X_train = X_Train.drop(['Clean'], axis =1)\n",
    "X_test = X_Test.drop(['Clean'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98aded7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if chosen_algorithm == \"NN\":\n",
    "    from sklearn.preprocessing import StandardScaler  \n",
    "    scaler = StandardScaler()  \n",
    "    # Don't cheat - fit only on training data\n",
    "    scaler.fit(X_train)  \n",
    "    X_train = scaler.transform(X_train)  \n",
    "    # apply same transformation to test data\n",
    "    X_test = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5053923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move this block to the location in the code from where you want to start tracking\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac7da4",
   "metadata": {},
   "source": [
    "Now choosing and fitting the ML algorithm you chose earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1daf7d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = chosen_algorithm\n",
    "clf = choose_algorithm(name).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8001bbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Used: 12570529792 bytes\n",
      "Time Elapsed: 23.910399436950684 seconds\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Move this block to where you want to stop the\n",
    "tracking of used time and memory of the code.\n",
    "'''\n",
    "\n",
    "#start_time = time.time()\n",
    "# Call the function to get memory usage and elapsed time\n",
    "memory_used, time_elapsed = track_memory_and_time(start_time)\n",
    "\n",
    "print(f\"Memory Used: {memory_used} bytes\")\n",
    "print(f\"Time Elapsed: {time_elapsed} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a93838",
   "metadata": {},
   "source": [
    "The block below prints out the accuracy of the ML algorithm on the test set.\n",
    "If used algorithm is the decision tree, it also prints out the visualization of the tree.\n",
    "\n",
    "<b>How could you check the accuracy? Think of a strategy depending on the used algorithm.</b>\n",
    "\n",
    "Consider cases with correct labels in the training data and cases with unclear cases in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "102dacc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the algorithm is: \n",
      "75.23510971786834%\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy of the algorithm is: \")\n",
    "print(accuracy(clf, X_test, y_test))\n",
    "if chosen_algorithm == \"Tree\":\n",
    "    tree.plot_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3cdc341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function that will identify when the algorithm diverged from the original label in the dataset\n",
    "def find_differences(clf, X_test, y_test):\n",
    "    \n",
    "    predictions = []\n",
    "    test_labels = []\n",
    "    column_names = list(X_test.columns.values)\n",
    "    events = pd.DataFrame(columns=list(X_test.columns.values))\n",
    "    \n",
    "    for i in range(len(list(y_test))-1):\n",
    "        predict = [X_test.drop(columns=\"Clean\", axis=1).values[i+1]]\n",
    "        prediction = str(clf.predict(predict)[0])\n",
    "        if str(clf.predict(predict)[0]) != list(y_test)[i+1]:\n",
    "            predictions.append(prediction)\n",
    "            test_labels.append(list(y_test)[i+1])\n",
    "            events.loc[len(events)] = X_test.values[i+1]\n",
    "            \n",
    "    df = pd.DataFrame(columns=['Prediction', 'Original label'])\n",
    "    df['Prediction']= predictions\n",
    "    df['Original label']= test_labels\n",
    "\n",
    "    df = pd.concat([df, events], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08278ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function that will find the documents that the algorithm diverged on\n",
    "def get_original_doc(differences):\n",
    "    df = differences(['Prediction', 'Original label', 'Clean'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b107c5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances in which the algorithm prediction diverged from the original label were: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Original label</th>\n",
       "      <th>Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Human capital</td>\n",
       "      <td>Scientific knowledge</td>\n",
       "      <td>All Rights Reserved   Section: International; Foreign Organizations Length: 532 words Byline: CERN - European Organization for Nuclear Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Human capital</td>\n",
       "      <td>Scientific knowledge</td>\n",
       "      <td>All Rights Reserved   Length: 294 words Body   European Organization for Nuclear Research (CERN) has awarded a contract for Research and Development Services and Related Consultancy Services at Switzerland-Geneva. The contract was awarded to ELYTT Energy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scientific knowledge</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Body   The CERN research board has approved the Forward Search Experiment, giving a green light to the assembly, installation and use of an instrument that will look for new fundamental particles at the Large Hadron Collider in Geneva, Switzerland. Initiated by physicists at the University of California, Irvine, the five-year FASER project is funded by grants of $1 million each from the Heising-Simons Foundation and the Simons Foundation - with additional support from CERN, the European Organization for Nuclear Research. FASER's focus is to find light, extremely weakly interacting particles that have so far eluded scientists, even in the high-energy experiments conducted at the CERN-operated LHC, the largest particle accelerator in the world.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Technology</td>\n",
       "      <td>Human capital</td>\n",
       "      <td>Body   (ANSA) - November 6 - CERN said Wednesday that its council has appointed Italian physicist Fabiola Gianotti for a second mandate as the Director-General of the European Organization for Nuclear Research. The 59-year-old Rome native, who participated in the discovery of the Higgs boson, became the first woman to head the lab when she started her first term in 2016.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scientific knowledge</td>\n",
       "      <td>Human capital</td>\n",
       "      <td>The history of Web sites  Over the past three months, I've been providing tips on how to construct and market a Web site but it wasn't always this easy. The ability to construct and promote a site has come a long way since 1989, when the World Wide Web was created by Tim Bemers-Lee, an English computer scientist for the European Organization for Nuclear Research (CERN). Four years later, on April 30, CERN announced that the World Wide Web would be free to use for anyone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Scientific knowledge</td>\n",
       "      <td>Human capital</td>\n",
       "      <td>On Aug. 28, an international team of thousands of researchers -- including Iashvili, Kharchilava and Rappoccio -- announced that they had observed the Higgs boson, a subatomic particle, decaying into a pair of lighter particles called a bottom quark and antibottom quark. The sighting took place at the world's most powerful particle accelerator, the Large Hadron Collider (LHC) at the European Organization for Nuclear Research (CERN). The finding deepens our understanding of why objects have mass.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Human capital</td>\n",
       "      <td>Scientific knowledge</td>\n",
       "      <td>Length: 348 words Body   Paris: United Nations Educational, Scientific and Cultural Organization has issued the following news release:  The European Organization for Nuclear Research (CERN) and the United Nations Educational, Scientific and Cultural Organization (UNESCO) will co-organize the UNESCO-CERN School on Digital Libraries 2018 which will be held in Nairobi, Kenya, from 8 to 12 October 2018 and will be hosted by Nairobi University.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Scientific knowledge</td>\n",
       "      <td>Human capital</td>\n",
       "      <td>Chapter eight returns to profiles, this time in the form of nine interviews of contemporary individuals. Each provides intriguing insights into curious personalities, such as physicist Freeman Dyson and his preference for details over the big picture; a fascination with multiple subjects on the part of Fabiola Gianotti, director-general of the European Organization for Nuclear Research (CERN); Brian May, guitarist of the rock band Queen, who later pursued an astrophysics career; and a belief that childhood represented a time of even greater, unrestrained curiosity, typified by Gianotti and autodidact Marilyn vos Savant. Livio recalls the many times societies have sought to limit curiosity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Scientific knowledge</td>\n",
       "      <td>Human capital</td>\n",
       "      <td>Cornell Chronicle University has issued the following news release:  Particle accelerators such as the Large Hadron Collider (LHC) at the European Organization for Nuclear Research (CERN) produce massive amounts of data that help answer long-held questions regarding Earth and the far reaches of the universe. The Higgs boson, which had been the missing link in the Standard Model of Particle Physics, was discovered there in 2012 and earned researchers the 2013 Nobel Prize in physics.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Technology</td>\n",
       "      <td>Human capital</td>\n",
       "      <td>She taught at Harvard University and worked at a super collider in Texas. Today, McBride leads the Compact Muon Solenoid collaboration at the Large Hadron Collider of the European Organization for Nuclear Research (CERN) lab in Geneva, Switzerland, and is a distinguished scientist at the Fermi National Accelerator Laboratory (Fermilab), the U.S. Department of Energy's premier particle physics lab near Chicago. McBride now calls Chicago home, but mostly lives in France near Geneva.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Prediction        Original label  \\\n",
       "0   Human capital         Scientific knowledge   \n",
       "1   Human capital         Scientific knowledge   \n",
       "2   Scientific knowledge  Technology             \n",
       "3   Technology            Human capital          \n",
       "4   Scientific knowledge  Human capital          \n",
       "..                   ...            ...          \n",
       "91  Scientific knowledge  Human capital          \n",
       "92  Human capital         Scientific knowledge   \n",
       "93  Scientific knowledge  Human capital          \n",
       "94  Scientific knowledge  Human capital          \n",
       "95  Technology            Human capital          \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Clean  \n",
       "0   All Rights Reserved   Section: International; Foreign Organizations Length: 532 words Byline: CERN - European Organization for Nuclear Research                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "1   All Rights Reserved   Length: 294 words Body   European Organization for Nuclear Research (CERN) has awarded a contract for Research and Development Services and Related Consultancy Services at Switzerland-Geneva. The contract was awarded to ELYTT Energy.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "2   Body   The CERN research board has approved the Forward Search Experiment, giving a green light to the assembly, installation and use of an instrument that will look for new fundamental particles at the Large Hadron Collider in Geneva, Switzerland. Initiated by physicists at the University of California, Irvine, the five-year FASER project is funded by grants of $1 million each from the Heising-Simons Foundation and the Simons Foundation - with additional support from CERN, the European Organization for Nuclear Research. FASER's focus is to find light, extremely weakly interacting particles that have so far eluded scientists, even in the high-energy experiments conducted at the CERN-operated LHC, the largest particle accelerator in the world.  \n",
       "3    Body   (ANSA) - November 6 - CERN said Wednesday that its council has appointed Italian physicist Fabiola Gianotti for a second mandate as the Director-General of the European Organization for Nuclear Research. The 59-year-old Rome native, who participated in the discovery of the Higgs boson, became the first woman to head the lab when she started her first term in 2016.                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "4   The history of Web sites  Over the past three months, I've been providing tips on how to construct and market a Web site but it wasn't always this easy. The ability to construct and promote a site has come a long way since 1989, when the World Wide Web was created by Tim Bemers-Lee, an English computer scientist for the European Organization for Nuclear Research (CERN). Four years later, on April 30, CERN announced that the World Wide Web would be free to use for anyone.                                                                                                                                                                                                                                                                                       \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ...                                                                                                                                                                                                                                                                                       \n",
       "91  On Aug. 28, an international team of thousands of researchers -- including Iashvili, Kharchilava and Rappoccio -- announced that they had observed the Higgs boson, a subatomic particle, decaying into a pair of lighter particles called a bottom quark and antibottom quark. The sighting took place at the world's most powerful particle accelerator, the Large Hadron Collider (LHC) at the European Organization for Nuclear Research (CERN). The finding deepens our understanding of why objects have mass.                                                                                                                                                                                                                                                              \n",
       "92  Length: 348 words Body   Paris: United Nations Educational, Scientific and Cultural Organization has issued the following news release:  The European Organization for Nuclear Research (CERN) and the United Nations Educational, Scientific and Cultural Organization (UNESCO) will co-organize the UNESCO-CERN School on Digital Libraries 2018 which will be held in Nairobi, Kenya, from 8 to 12 October 2018 and will be hosted by Nairobi University.                                                                                                                                                                                                                                                                                                                      \n",
       "93  Chapter eight returns to profiles, this time in the form of nine interviews of contemporary individuals. Each provides intriguing insights into curious personalities, such as physicist Freeman Dyson and his preference for details over the big picture; a fascination with multiple subjects on the part of Fabiola Gianotti, director-general of the European Organization for Nuclear Research (CERN); Brian May, guitarist of the rock band Queen, who later pursued an astrophysics career; and a belief that childhood represented a time of even greater, unrestrained curiosity, typified by Gianotti and autodidact Marilyn vos Savant. Livio recalls the many times societies have sought to limit curiosity.                                                        \n",
       "94   Cornell Chronicle University has issued the following news release:  Particle accelerators such as the Large Hadron Collider (LHC) at the European Organization for Nuclear Research (CERN) produce massive amounts of data that help answer long-held questions regarding Earth and the far reaches of the universe. The Higgs boson, which had been the missing link in the Standard Model of Particle Physics, was discovered there in 2012 and earned researchers the 2013 Nobel Prize in physics.                                                                                                                                                                                                                                                                           \n",
       "95  She taught at Harvard University and worked at a super collider in Texas. Today, McBride leads the Compact Muon Solenoid collaboration at the Large Hadron Collider of the European Organization for Nuclear Research (CERN) lab in Geneva, Switzerland, and is a distinguished scientist at the Fermi National Accelerator Laboratory (Fermilab), the U.S. Department of Energy's premier particle physics lab near Chicago. McBride now calls Chicago home, but mostly lives in France near Geneva.                                                                                                                                                                                                                                                                             \n",
       "\n",
       "[96 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Instances in which the algorithm prediction diverged from the original label were: \")\n",
    "df = find_differences(clf, X_Test, y_test)[['Prediction', 'Original label', 'Clean']]\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59081aa3",
   "metadata": {},
   "source": [
    "<b>How do you know if this is to be agreed with or not? How is this different from a case with quantitative data? How would you go about improving the process and how would you get the resourches for this?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd012134",
   "metadata": {},
   "source": [
    "Read on the __[energy use and co2](https://medium.com/stanford-magazine/carbon-and-the-cloud-d6f481b79dfe)__ effects of different types of AI. \n",
    "\n",
    "Briefly summarized, algorithms ran on a personal hard disk, require about 0.000005 kWh per gigabyte to save your data, whereas the combination of transmitting your data and storing it in a data center probably requires about 3 to 7 kWh per gigabyte. Moreover, storing 100 gigabytes of data in the cloud during a year releases 0.2 tons of CO2.\n",
    "\n",
    "Are you currently running your algorithm in the cloud of on your personal device? Based on the memory use trackers you have used in the code, calculate how much more or less (in %) doing the opposite would do.\n",
    "\n",
    "Who bears the costs of the pollution? Estimate the costs of pollution and consider a scenario where the data storer and user would bear the costs relevant to this. \n",
    "\n",
    "In addition, consider the rare minerals and their associated pollution required for an average server farm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b69375a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
